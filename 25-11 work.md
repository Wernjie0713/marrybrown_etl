# Work Log: November 25, 2025

## Overview
This document records the fixes and improvements made to the MarryBrown ETL pipeline on November 25, 2025.

---

## 1. Fixed Critical SQL Errors in `transform_api_to_facts.py`

### Issue 1: Duplicate SaleNumber Definition
**Problem:** The `TransformedData` CTE had `SaleNumber` defined twice in its SELECT list, causing SQL error: "The column 'SaleNumber' was specified multiple times for 'TransformedData'."

**Location:** `api_etl/transform_api_to_facts.py` - `transform_to_facts_optimized()` function

**Fix:** Removed the duplicate `SaleNumber` definition from the `TransformedData` CTE. The `SaleNumber` is now defined once as:
```sql
CAST(cs.SaleID AS VARCHAR(45)) as SaleNumber
```

**Impact:** Resolved SQL compilation error that prevented the MERGE statement from executing.

---

### Issue 2: MAX() Operator on BIT Data Type
**Problem:** The `AggregatedData` CTE attempted to use `MAX()` directly on `IsFOC` and `IsServiceCharge` columns, which are of `bit` data type in SQL Server. This caused error: "Operand data type bit is invalid for max operator."

**Location:** `api_etl/transform_api_to_facts.py` - `AggregatedData` CTE (lines 239, 242)

**Fix:** Cast `bit` columns to `INT` before applying `MAX()`, then cast the result back to `BIT`:
```sql
-- Before (ERROR):
MAX(IsFOC) as IsFOC,
MAX(IsServiceCharge) as IsServiceCharge

-- After (FIXED):
CAST(MAX(CAST(IsFOC AS INT)) AS BIT) as IsFOC,
CAST(MAX(CAST(IsServiceCharge AS INT)) AS BIT) as IsServiceCharge
```

**Impact:** Resolved SQL Server data type error, allowing aggregation to work correctly on boolean fields.

---

## 2. Comprehensive Code Review and Fixes

A thorough code review was conducted on both `extract_fast_sample.py` and `transform_api_to_facts.py`, identifying and fixing multiple issues:

### 2.1 SQL Injection Vulnerability (CRITICAL)
**File:** `api_etl/extract_fast_sample.py`  
**Location:** `get_location_keys_batch()` function (lines 215-218)

**Problem:** String interpolation in SQL queries was vulnerable to SQL injection attacks.

**Fix:** Replaced string interpolation with parameterized queries using SQLAlchemy's `text()` with named parameters:
```python
# Before (VULNERABLE):
outlets_str = "','".join(outlets_list)
query = f"""
    SELECT LocationName, LocationKey FROM dim_locations
    WHERE LocationName IN ('{outlets_str}')
"""

# After (SECURE):
query = text(f"""
    SELECT LocationName, LocationKey FROM dim_locations
    WHERE LocationName IN ({', '.join([f':outlet_{i}' for i in range(len(outlets_list))])})
""")
params = {f'outlet_{i}': name for i, name in enumerate(outlets_list)}
result = conn.execute(query, params).fetchall()
```

---

### 2.2 Incorrect Time Calculation (HIGH)
**File:** `api_etl/transform_api_to_facts.py`  
**Location:** `transform_to_facts_optimized()` function (line 319)

**Problem:** `chunk_time` was calculated as total elapsed time instead of per-chunk time.

**Fix:** Added `chunk_start_time` tracking before processing each chunk:
```python
chunk_start_time = time.time()
# ... process chunk ...
chunk_time = time.time() - chunk_start_time  # Accurate per-chunk timing
```

---

### 2.3 CostAmount Standardization (HIGH)
**File:** `api_etl/transform_api_to_facts.py`  
**Location:** `transform_to_facts()` and `transform_to_facts_for_period()` functions

**Problem:** Legacy functions were calculating `CostAmount` as `si.Cost * si.Quantity` instead of using the pre-calculated `si.CostAmount` from staging.

**Fix:** Updated to use `si.CostAmount` directly:
```python
# Before:
si.Cost * si.Quantity * pa.allocation_percentage

# After:
si.CostAmount * pa.allocation_percentage
```

**Impact:** Ensures consistent cost calculation across all transformation paths.

---

### 2.4 Improved Error Handling (HIGH)
**File:** `api_etl/extract_fast_sample.py`  
**Location:** `write_parallel()` function (lines 767-769)

**Fix:** Added `traceback.print_exc()` for detailed error logging:
```python
except Exception as e:
    print(f"  [ERROR] Failed to write {table_name}: {e}")
    traceback.print_exc()  # Added for detailed error visibility
    results[table_name] = 0
```

**File:** `api_etl/transform_api_to_facts.py`  
**Location:** `transform_to_facts_optimized()` function

**Fix:** Wrapped `cleanup_staging()` call in try-except to prevent cleanup failures from crashing the main ETL job:
```python
try:
    cleanup_staging(STAGING_RETENTION_DAYS)
except Exception as e:
    print(f"[WARNING] Cleanup failed (non-fatal): {e}")
```

---

### 2.5 Code Quality Improvements (MEDIUM/LOW)
- **Removed unused imports:**
  - `extract_fast_sample.py`: Removed `NullPool`, `Decimal`, `InvalidOperation`, `math`
  - `transform_api_to_facts.py`: Removed `DataQualityValidator`

- **Shared Database Utility:**
  - Created `utils/db_connection.py` to centralize `get_warehouse_engine()` function
  - Both ETL scripts now import from the shared utility, improving maintainability

- **Type Hints:**
  - Added return type hints to key functions for better code documentation

---

## 3. Fixed Environment Loading Duplication

### Problem
When running `transform_api_to_facts.py`, the environment was being loaded twice:
1. First load: `load_environment(force_local=True)` → loads `.env.local`
2. Second load: `get_warehouse_engine()` → calls `load_environment()` without parameters → loads `.env.cloud` and overwrites `.env.local` settings

**Terminal Output:**
```
[INFO] Loaded environment from: C:\laragon\www\marrybrown_etl\.env.local (local mode)
...
[INFO] Loaded environment from: C:\laragon\www\marrybrown_etl\.env.cloud
```

### Solution
**File:** `utils/env_loader.py`

Made `load_environment()` function **idempotent** by adding module-level flags to track loading state:

```python
# Module-level flag to track if environment has been loaded
_env_loaded = False
_env_loaded_mode = None  # 'local' or 'cloud' or None

def load_environment(force_local=False):
    global _env_loaded, _env_loaded_mode
    
    # If already loaded, don't reload (idempotent)
    if _env_loaded:
        if _env_loaded_mode == 'local':
            return  # Never reload if already loaded with force_local=True
        return  # Don't reload if already loaded
    
    # ... rest of loading logic ...
```

**Key Changes:**
- Added `override=False` parameter to `load_dotenv()` calls to prevent overwriting existing environment variables
- Early return if environment is already loaded
- Tracks loading mode (`local` or `cloud`) to prevent mode switching

**Impact:**
- Only one `[INFO] Loaded environment from:` message appears
- `.env.local` settings are preserved when `force_local=True` is called first
- Prevents accidental environment variable overwrites

---

## 4. Fixed Calendar Date Picker Year/Month Selection (Portal)

### Problem
**File:** `marrybrown-portal/src/components/ui/date-picker.jsx`  
**Location:** `DatePicker` component

The calendar date picker only allowed users to navigate month-by-month using previous/next arrows. Users could not directly select a year or month, making it cumbersome to navigate to dates far in the past or future (e.g., selecting October 2018 required clicking through many months).

**User Experience Issue:**
- Users had to click through months sequentially to reach desired dates
- No direct year selection capability
- Year was not clickable/selectable as a dropdown

### Solution
Updated the `DatePicker` component to use `captionLayout="dropdown"` prop on the `Calendar` component, which enables the built-in month and year dropdown selectors from `react-day-picker`.

**Fix:**
```jsx
// Before:
<Calendar
  mode="single"
  selected={date}
  onSelect={onDateChange}
  initialFocus
/>

// After:
<Calendar
  mode="single"
  selected={date}
  onSelect={onDateChange}
  initialFocus
  captionLayout="dropdown"  // Added to enable month/year dropdowns
/>
```

**Location:** `marrybrown-portal/src/components/ui/date-picker.jsx` (line 37)

**Impact:**
- Users can now directly select year from a dropdown (e.g., "2025", "2024", "2018")
- Users can directly select month from a dropdown (e.g., "Jan", "Feb", "Oct")
- Matches the official react-day-picker documentation pattern
- All date pickers throughout the portal (Daily Sales, Product Mix, EOD Summary) automatically benefit from this improvement

**Note:** The fix was applied at the component usage level (in `date-picker.jsx`) rather than modifying the base `Calendar` component itself, following the official documentation pattern and keeping the base component clean and reusable.

---

## 5. Data Quality Investigation & Deduplication (Sep–Nov 2018)

### 5.1 How We Tested
- **Automation:** `marrybrown_api/investigate_duplicates.py` now orchestrates six diagnostic SQL queries through SQLAlchemy. It inspects TransactionKey uniqueness, SaleNumber duplication per date/location, drill-downs for MB Angsana on 2018‑10‑01, per-sale vs per-row totals, Return transactions, and SaleType breakdowns.
- **Custom SQL snippets:** Beyond the script we ran ad-hoc SQL (via temporary Python helpers) to:
  - Aggregate facts by the true grain (`SaleNumber + DateKey + ProductKey + PaymentTypeKey`) and count rows > 1.
  - Compare line-level totals to per-sale totals (`SUM(TotalAmount)` vs `SUM` after grouping by SaleNumber) to ensure dedupe did not change net value.
  - Produce month/location “rows-per-sale” ratios to spot anomalous periods.
- **Columns of interest:** SaleNumber, DateKey, LocationKey, ProductKey, PaymentTypeKey, Quantity, TotalAmount, CostAmount, SaleType, SalesStatus, OrderSource, CardType, TaxCode, TaxRate, IsFOC, Rounding, Model, IsServiceCharge.

### 5.2 Findings
- TransactionKey remained unique; duplication happened because the same sale/product/payment rows were inserted multiple times during older ETL runs.
- Sep–Nov 2018 showed huge duplication (e.g., Oct: 6,409 duplicate grain rows; Nov: 11,137) whereas Dec 2018 onward is clean (0 duplicates at that grain).
- Example: Sale `901875` on 2018‑10‑27 at MB IOI Kulai had 18 identical rows for the same product/payment.
- After dedupe, MB Angsana 2018‑10‑01 now reports RM 7,016.22 for both line-level and per-sale calculations, with **0** duplicate grain rows; however, Xilnex still reports RM ≈2k, so the remaining discrepancy is a business-rule difference rather than raw duplication.
- April 2019 data already matches Xilnex much better, so we agreed to validate against that month while planning a full re-extract for the earlier window.

### 5.3 Deduplication SQL Used

```sql
BEGIN TRAN;

IF OBJECT_ID('tempdb..#CleanFacts') IS NOT NULL DROP TABLE #CleanFacts;

SELECT
    SaleNumber,
    DateKey,
    MIN(TimeKey)              AS TimeKey,
    MIN(LocationKey)          AS LocationKey,
    ProductKey,
    MIN(CustomerKey)          AS CustomerKey,
    MIN(StaffKey)             AS StaffKey,
    MIN(PromotionKey)         AS PromotionKey,
    PaymentTypeKey,
    MIN(TerminalKey)          AS TerminalKey,
    MIN(SaleType)             AS SaleType,
    MIN(SubSalesType)         AS SubSalesType,
    MIN(SalesStatus)          AS SalesStatus,
    MIN(OrderSource)          AS OrderSource,
    SUM(Quantity)             AS Quantity,
    SUM(GrossAmount)          AS GrossAmount,
    SUM(DiscountAmount)       AS DiscountAmount,
    SUM(NetAmount)            AS NetAmount,
    SUM(TaxAmount)            AS TaxAmount,
    SUM(TotalAmount)          AS TotalAmount,
    SUM(CostAmount)           AS CostAmount,
    MAX(CardType)             AS CardType,
    MAX(TaxCode)              AS TaxCode,
    MAX(TaxRate)              AS TaxRate,
    CAST(MAX(CAST(IsFOC AS INT)) AS BIT)          AS IsFOC,
    SUM(Rounding)             AS Rounding,
    MAX(Model)                AS Model,
    CAST(MAX(CAST(IsServiceCharge AS INT)) AS BIT) AS IsServiceCharge
INTO #CleanFacts
FROM dbo.fact_sales_transactions
WHERE DateKey BETWEEN 20180901 AND 20181130
GROUP BY SaleNumber, DateKey, ProductKey, PaymentTypeKey;

DELETE FROM dbo.fact_sales_transactions
WHERE DateKey BETWEEN 20180901 AND 20181130;

INSERT INTO dbo.fact_sales_transactions (
    SaleNumber, DateKey, TimeKey, LocationKey, ProductKey, CustomerKey,
    StaffKey, PromotionKey, PaymentTypeKey, TerminalKey, SaleType,
    SubSalesType, SalesStatus, OrderSource, Quantity, GrossAmount,
    DiscountAmount, NetAmount, TaxAmount, TotalAmount, CostAmount,
    CardType, TaxCode, TaxRate, IsFOC, Rounding, Model, IsServiceCharge
)
SELECT
    SaleNumber, DateKey, TimeKey, LocationKey, ProductKey, CustomerKey,
    StaffKey, PromotionKey, PaymentTypeKey, TerminalKey, SaleType,
    SubSalesType, SalesStatus, OrderSource, Quantity, GrossAmount,
    DiscountAmount, NetAmount, TaxAmount, TotalAmount, CostAmount,
    CardType, TaxCode, TaxRate, IsFOC, Rounding, Model, IsServiceCharge
FROM #CleanFacts;

COMMIT TRAN;
```

### 5.4 Current Plan
- Oct/Nov 2018 still don’t align numerically with Xilnex despite dedupe, so we will:
  - Validate the portal/API against Apr 2019 (stable data) first.
  - Schedule a full ETL re-extract for Sep–Nov 2018 (reset checkpoints, rerun extraction + transformation) once bandwidth allows.
- Until the re-extract happens, treat those months as informational only.

---

## Files Modified

1. `api_etl/transform_api_to_facts.py`
   - Fixed duplicate `SaleNumber` definition
   - Fixed `MAX()` on `bit` data types
   - Fixed time calculation
   - Standardized `CostAmount` calculation
   - Improved error handling for cleanup

2. `api_etl/extract_fast_sample.py`
   - Fixed SQL injection vulnerability
   - Improved error logging with traceback
   - Removed unused imports
   - Updated to use shared database utility

3. `utils/env_loader.py`
   - Made `load_environment()` idempotent
   - Added module-level state tracking
   - Prevented duplicate environment loading

4. `utils/db_connection.py` (New)
   - Centralized database connection logic
   - Shared utility for consistent connection pooling

5. `marrybrown-portal/src/components/ui/date-picker.jsx`
   - Added `captionLayout="dropdown"` to enable year/month dropdowns
   - Improved date picker UX for selecting dates far in past/future

---

## Testing Recommendations

1. **Verify MERGE Operations:**
   - Run `transform_api_to_facts.py` and confirm no duplicate row errors
   - Spot-check sales with split tenders to ensure grouped totals match staging detail

2. **Verify Environment Loading:**
   - Run scripts and confirm only one environment load message appears
   - Verify `.env.local` settings are used when `force_local=True`

3. **Verify SQL Injection Fix:**
   - Test `extract_fast_sample.py` with various outlet names (including special characters)

4. **Verify Error Handling:**
   - Test cleanup failures don't crash the main ETL job
   - Verify detailed error traces appear in logs

---

## Summary

Today's work focused on:
- ✅ Resolving critical SQL errors preventing MERGE operations
- ✅ Addressing security vulnerabilities (SQL injection)
- ✅ Improving code quality and maintainability
- ✅ Fixing environment loading duplication issue
- ✅ Standardizing data calculations across transformation paths
- ✅ Enhancing portal UX with year/month dropdown selectors in date pickers
- ✅ Investigating Sep–Nov 2018 duplicate facts, running in-place deduplication, and documenting that totals still disagree with Xilnex; agreed to pause validation on those months and verify against Apr 2019 first (plan: clear and rerun ETL later)

All fixes have been implemented and are ready for testing.

